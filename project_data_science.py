# -*- coding: utf-8 -*-
"""project1 data science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zht4Yg8pAvucUG__lzUzRI4WlqXS_xpg

# **Enhancing Real Estate Investment Decisions with Predictive Modeling**
"""

from google.colab import files
uploaded_files = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Load datasets
train_df = pd.read_csv('Housing-project-train-data.csv')
test_df = pd.read_csv('Hosuing-project-test-data.csv')

# Shape of datasets
print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)

# Display the first few rows
train_df.head()

# Check data types and non-null counts
train_df.info()

# Find missing values in each column
missing_values = train_df.isnull().sum()
missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

# Display missing value count and percentage
missing_percent = (missing_values / len(train_df)) * 100
missing_summary = pd.DataFrame({'Missing Count': missing_values, 'Missing %': missing_percent})
print(missing_summary)

# Quick overview of numeric features
train_df.describe().T

# Count and percentage of missing values
missing_values = train_df.isnull().sum()
missing_values = missing_values[missing_values > 0]
missing_percent = (missing_values / len(train_df)) * 100

# Combine into a DataFrame
missing_data = pd.DataFrame({'Missing Count': missing_values, 'Missing %': missing_percent})
missing_data = missing_data.sort_values(by='Missing %', ascending=False)

# Show top missing features
print("Features with missing values:")
missing_data.head(20)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.barplot(x=missing_data.head(20).index, y=missing_data.head(20)['Missing %'])
plt.xticks(rotation=90)
plt.ylabel("Percentage of Missing Values")
plt.title("Top Features with Missing Values")
plt.show()

# 1. MasVnrArea: Fill missing values with 0 (means no veneer)
train_df['MasVnrArea'] = train_df['MasVnrArea'].fillna(0)

# 2. GarageYrBlt: Fill with YearBuilt only where GarageType is 'None'
# (ensures alignment with GarageType cleaning already done)
train_df['GarageYrBlt'] = train_df['GarageYrBlt'].fillna(train_df['YearBuilt'])

# 3. LotFrontage: Fill missing values using Neighborhood-specific median
train_df['LotFrontage'] = train_df.groupby('Neighborhood')['LotFrontage'].transform(
    lambda x: x.fillna(x.median())
)

# Threshold for dropping columns (e.g., > 15% missing)
missing_threshold = 15

# Calculate missing percentages
missing_percent = train_df.isnull().sum() / len(train_df) * 100

# Filter columns that exceed the threshold
high_missing_cols = missing_percent[missing_percent > missing_threshold].index.tolist()

print("Columns with more than 15% missing values:")
print(high_missing_cols)

# Drop columns with high missing values
train_df = train_df.drop(columns=high_missing_cols)

# Identify low-variance features
low_variance_cols = [col for col in train_df.columns if train_df[col].nunique() == 1]

print("Low-variance columns:")
print(low_variance_cols)

# Drop them
train_df = train_df.drop(columns=low_variance_cols)

# Distribution plot of SalePrice
plt.figure(figsize=(10, 6))
sns.histplot(train_df['SalePrice'], kde=True, bins=30)
plt.title('Distribution of SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.show()

# Check skewness and kurtosis
print("Skewness:", train_df['SalePrice'].skew())
print("Kurtosis:", train_df['SalePrice'].kurt())

# Select numeric columns
numeric_feats = train_df.select_dtypes(include=['int64', 'float64'])

# Compute correlation matrix with SalePrice
correlation = numeric_feats.corr()
top_corr_features = correlation['SalePrice'].abs().sort_values(ascending=False).head(15)

# Plot heatmap of top correlated features
plt.figure(figsize=(10, 8))
sns.heatmap(train_df[top_corr_features.index].corr(), annot=True, cmap='coolwarm')
plt.title('Top Correlated Features with SalePrice')
plt.show()

# Scatter plot for GrLivArea vs SalePrice
plt.figure(figsize=(8, 6))
sns.scatterplot(x='GrLivArea', y='SalePrice', data=train_df)
plt.title('GrLivArea vs SalePrice')
plt.show()

# Boxplot for OverallQual vs SalePrice
plt.figure(figsize=(8, 6))
sns.boxplot(x='OverallQual', y='SalePrice', data=train_df)
plt.title('OverallQual vs SalePrice')
plt.show()

# Total square footage
train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']

# Age of the house at time of sale
train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']

# Years since last remodel
train_df['YearsSinceRemodel'] = train_df['YrSold'] - train_df['YearRemodAdd']

# Total bathrooms
train_df['TotalBath'] = (
    train_df['FullBath'] +
    (0.5 * train_df['HalfBath']) +
    train_df['BsmtFullBath'] +
    (0.5 * train_df['BsmtHalfBath'])
)

# Handle skewed numeric features
numeric_feats = train_df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1)
skewed_feats = numeric_feats.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
high_skew = skewed_feats[skewed_feats > 0.75]

# Log-transform skewed features
for col in high_skew.index:
    train_df[col] = np.log1p(train_df[col])

# Also log-transform SalePrice to normalize it
train_df['SalePrice'] = np.log1p(train_df['SalePrice'])

# Separate features and target
X = train_df.drop(columns=['SalePrice'])
y = train_df['SalePrice']

# One-hot encode categorical columns
X = pd.get_dummies(X, drop_first=True)


# Train-test split
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)


# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)

Lasso(alpha=0.001, max_iter=10000)

models = {
    "Linear": LinearRegression(),
    "Ridge": Ridge(alpha=10),
    "Lasso": Lasso(alpha=0.01, max_iter=10000),
    "RandomForest": RandomForestRegressor(n_estimators=100, random_state=42)
}
# Helper to evaluate models
def evaluate_model(model, X_val, y_val):
    preds = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, preds))
    r2 = r2_score(y_val, preds)
    return rmse, r2

models = {
    "Linear": LinearRegression(),
    "Ridge": Ridge(alpha=10),
    "Lasso": Lasso(alpha=0.001),
    "RandomForest": RandomForestRegressor(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    rmse, r2 = evaluate_model(model, X_valid_scaled, y_valid)
    results[name] = {'RMSE': rmse, 'R2': r2}

# Print results
print("\nModel Performance:")
for model, scores in results.items():
    print(f"{model}: RMSE = {scores['RMSE']:.4f}, R2 = {scores['R2']:.4f}")